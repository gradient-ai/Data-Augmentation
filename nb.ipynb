{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  article dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as Datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm as tqdm_regular\n",
    "import seaborn as sns\n",
    "from torchvision.utils import make_grid\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  setting up device\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "  print('Running on the GPU')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "  print('Running on the CPU')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Cropping"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def random_crop(dataset: list, crop_size=(20, 20)):\n",
    "  \"\"\"\n",
    "  This function replicates the random crop process\n",
    "  \"\"\"\n",
    "  cropped = []\n",
    "  images = [x[0] for x in dataset]\n",
    "  for image in tqdm_regular(images):\n",
    "    # deriving image size\n",
    "    img_size = image.shape\n",
    "\n",
    "    #  extracting channels\n",
    "    channel_0, channel_1, channel_2 = image[:,:,0], image[:,:,1], image[:,:,2]\n",
    "\n",
    "    #  deriving random indicies\n",
    "    idx_row = random.randint(0, img_size[0] - crop_size[0])\n",
    "    idx_column = random.randint(0, img_size[0] - crop_size[0])\n",
    "\n",
    "    #  cropping image per channel\n",
    "    channel_0 = channel_0[idx_row:idx_row + crop_size[0], \n",
    "                          idx_column:idx_column + crop_size[1]]\n",
    "    channel_1 = channel_1[idx_row:idx_row + crop_size[0], \n",
    "                          idx_column:idx_column + crop_size[1]]\n",
    "    channel_2 = channel_2[idx_row:idx_row + crop_size[0], \n",
    "                          idx_column:idx_column + crop_size[1]]\n",
    "\n",
    "    #  stacking images\n",
    "    image = np.dstack((channel_0, channel_1, channel_2))\n",
    "\n",
    "    #  resizing image\n",
    "    image = cv2.resize(image, (32, 32))\n",
    "    #  labelling and appending to list\n",
    "    cropped.append((image, 1))\n",
    "  return cropped "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Image noising"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def noise_image(dataset: list, noise_intensity=0.2):\n",
    "  \"\"\"\n",
    "  This function replicates the image noising process\n",
    "  \"\"\"\n",
    "  noised = []\n",
    "  noise_threshold = 1 - noise_intensity\n",
    "  images = [x[0] for x in dataset]\n",
    "\n",
    "  for image in tqdm_regular(images):\n",
    "    #  extracting channels\n",
    "    channel_0, channel_1, channel_2 = image[:,:,0], image[:,:,1], image[:,:,2]\n",
    "\n",
    "    #  flatenning channels\n",
    "    channel_0 = channel_0.reshape(1024)\n",
    "    channel_1 = channel_1.reshape(1024)\n",
    "    channel_2 = channel_2.reshape(1024)\n",
    "\n",
    "    #  creating vector of zeros\n",
    "    noise_0 = np.zeros(1024, dtype='uint8')\n",
    "    noise_1 = np.zeros(1024, dtype='uint8')\n",
    "    noise_2 = np.zeros(1024, dtype='uint8')\n",
    "\n",
    "    #  noise probability\n",
    "    for idx in range(1024):\n",
    "      regulator = round(random.random(), 1)\n",
    "      if regulator > noise_threshold:\n",
    "        noise_0[idx] = 255\n",
    "        noise_1[idx] = 255\n",
    "        noise_2[idx] = 255\n",
    "      elif regulator == noise_threshold:\n",
    "        noise_0[idx] = 0\n",
    "        noise_1[idx] = 0\n",
    "        noise_2[idx] = 0\n",
    "      else:\n",
    "        noise_0[idx] = channel_0[idx]\n",
    "        noise_1[idx] = channel_1[idx]\n",
    "        noise_2[idx] = channel_2[idx]\n",
    "    \n",
    "    #  reshaping noise vectors\n",
    "    noise_0 = noise_0.reshape((32, 32))\n",
    "    noise_1 = noise_1.reshape((32, 32))\n",
    "    noise_2 = noise_2.reshape((32, 32))\n",
    "\n",
    "    #  stacking images\n",
    "    image = np.dstack((noise_0, noise_1, noise_2))\n",
    "    #  labelling and appending to list\n",
    "    noised.append((image, 1))\n",
    "  return noised"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Image flipping"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def flip_image(dataset: list):\n",
    "  \"\"\"\n",
    "  This function replicates the process of horizontal flipping\n",
    "  \"\"\"\n",
    "  flipped = []\n",
    "  images = [x[0] for x in dataset]\n",
    "\n",
    "  for image in tqdm_regular(images):\n",
    "    #  extracting channels\n",
    "    channel_0, channel_1, channel_2 = image[:,:,0], image[:,:,1], image[:,:,2]\n",
    "\n",
    "    channel_0 = channel_0[:, ::-1]\n",
    "    channel_1 = channel_1[:, ::-1]\n",
    "    channel_2 = channel_2[:, ::-1]\n",
    "\n",
    "    #  stacking images\n",
    "    image = np.dstack((channel_0, channel_1, channel_2))\n",
    "    #  labelling and appending to list\n",
    "    flipped.append((image, 1))\n",
    "  return flipped"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Image Blurring"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def blur_image(dataset, kernel_size=5, padding=True):\n",
    "  \"\"\"This function performs convolution over an image\n",
    "   with the aim of blurring\"\"\"\n",
    "\n",
    "  #  defining internal function for padding\n",
    "  def pad_image(image, padding=2):\n",
    "    \"\"\"\n",
    "    This function performs zero padding using the number of \n",
    "    padding layers supplied as argument and return the padded\n",
    "    image.\n",
    "    \"\"\"\n",
    "    #  extracting channels\n",
    "    channel_0, channel_1, channel_2 = image[:,:,0], image[:,:,1], image[:,:,2]\n",
    "\n",
    "    #  creating an array of zeros\n",
    "    padded_0 = np.zeros((image.shape[0] + padding*2, \n",
    "                         image.shape[1] + padding*2), dtype='uint8')\n",
    "    padded_1 = np.zeros((image.shape[0] + padding*2, \n",
    "                         image.shape[1] + padding*2), dtype='uint8')\n",
    "    padded_2 = np.zeros((image.shape[0] + padding*2, \n",
    "                         image.shape[1] + padding*2), dtype='uint8')\n",
    "    \n",
    "    #  inserting image into zero array\n",
    "    padded_0[int(padding):-int(padding), \n",
    "             int(padding):-int(padding)] = channel_0\n",
    "    padded_1[int(padding):-int(padding), \n",
    "             int(padding):-int(padding)] = channel_1\n",
    "    padded_2[int(padding):-int(padding), \n",
    "             int(padding):-int(padding)] = channel_2\n",
    "\n",
    "    #  stacking images\n",
    "    padded = np.dstack((padded_0, padded_1, padded_2))\n",
    "\n",
    "    return padded\n",
    "\n",
    "  #  defining list to hold blurred images\n",
    "  all_blurred = []\n",
    "\n",
    "  #  defining gaussian 5x5 filter\n",
    "  gauss_5 = np.array([[1, 4, 7, 4, 1],\n",
    "                     [4, 16, 26, 16, 4],\n",
    "                     [7, 26, 41, 26, 7],\n",
    "                     [4, 16, 26, 16, 4],\n",
    "                     [1, 4, 7, 4, 1]])\n",
    "\n",
    "  filter = 1/273 * gauss_5\n",
    "  \n",
    "  #  extracting images\n",
    "  images = [x[0] for x in dataset]\n",
    "\n",
    "  for image in tqdm_regular(images):\n",
    "    if padding:\n",
    "      image = pad_image(image)\n",
    "    else:\n",
    "      image = image\n",
    "\n",
    "    #  extracting channels\n",
    "    channel_0, channel_1, channel_2 = image[:,:,0], image[:,:,1], image[:,:,2]\n",
    "\n",
    "    #  creating an array to store convolutions\n",
    "    blurred_0 = np.zeros(((image.shape[0] - kernel_size) + 1, \n",
    "                          (image.shape[1] - kernel_size) + 1), dtype='uint8')\n",
    "    blurred_1 = np.zeros(((image.shape[0] - kernel_size) + 1, \n",
    "                          (image.shape[1] - kernel_size) + 1), dtype='uint8')\n",
    "    blurred_2 = np.zeros(((image.shape[0] - kernel_size) + 1, \n",
    "                          (image.shape[1] - kernel_size) + 1), dtype='uint8')\n",
    "    \n",
    "    #  performing convolution\n",
    "    for i in range(image.shape[0]):\n",
    "      for j in range(image.shape[1]):\n",
    "        try:\n",
    "          blurred_0[i,j] = (channel_0[i:(i+kernel_size), j:(j+kernel_size)] * filter).sum()\n",
    "        except Exception:\n",
    "          pass\n",
    "\n",
    "    for i in range(image.shape[0]):\n",
    "      for j in range(image.shape[1]):\n",
    "        try:\n",
    "          blurred_1[i,j] = (channel_1[i:(i+kernel_size), j:(j+kernel_size)] * filter).sum()\n",
    "        except Exception:\n",
    "          pass\n",
    "\n",
    "    for i in range(image.shape[0]):\n",
    "      for j in range(image.shape[1]):\n",
    "        try:\n",
    "          blurred_2[i,j] = (channel_2[i:(i+kernel_size), j:(j+kernel_size)] * filter).sum()\n",
    "        except Exception:\n",
    "          pass\n",
    "\n",
    "    #  stacking images\n",
    "    blurred = np.dstack((blurred_0, blurred_1, blurred_2))\n",
    "    #  labelling and appending to list\n",
    "    all_blurred.append((blurred, 1))\n",
    "\n",
    "  return all_blurred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Putting it all together"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  loading training data\n",
    "training_set = Datasets.CIFAR10(root='./', download=True,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "#  loading validation data\n",
    "validation_set = Datasets.CIFAR10(root='./', download=True, train=False,\n",
    "                                transform=transforms.ToTensor())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def extract_images(dataset):\n",
    "  \"\"\"\n",
    "  This function helps to extract cat and dog images\n",
    "  from the cifar-10 dataset\n",
    "  \"\"\"\n",
    "  cats = []\n",
    "  dogs = []\n",
    "\n",
    "  for idx in tqdm_regular(range(len(dataset))):\n",
    "    if dataset.targets[idx]==3:\n",
    "      cats.append((dataset.data[idx], 0))\n",
    "    elif dataset.targets[idx]==5:\n",
    "      dogs.append((dataset.data[idx], 1))\n",
    "    else:\n",
    "      pass\n",
    "  return cats, dogs\n",
    "  \n",
    "#  extracting from the training set\n",
    "train_cats, train_dogs = extract_images(training_set)\n",
    "#  extracting from the validation set\n",
    "val_cats, val_dogs = extract_images(validation_set)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  deriving images of interest\n",
    "dog_images = train_dogs[:1200]\n",
    "\n",
    "#  creating random cropped copies\n",
    "dog_cropped = random_crop(dog_images)\n",
    "\n",
    "#  creating flipped copies\n",
    "dog_flipped = flip_image(dog_images)\n",
    "\n",
    "#  creating noised copies\n",
    "dog_noised = noise_image(dog_images)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Piecing together a dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  creating a dataset of 4,800 dog images\n",
    "train_dogs = dog_images + dog_cropped + dog_flipped + dog_noised\n",
    "\n",
    "#  instantiating training data\n",
    "training_images = train_cats[:4800] + train_dogs\n",
    "random.shuffle(training_images)\n",
    "\n",
    "#  instantiating validation data\n",
    "validation_images = val_cats + val_dogs\n",
    "random.shuffle(validation_images)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  defining dataset class\n",
    "class CustomCatsvsDogs(Dataset):\n",
    "  def __init__(self, data, transforms=None):\n",
    "    self.data = data\n",
    "    self.transforms = transforms\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    image = self.data[idx][0]\n",
    "    label = torch.tensor(self.data[idx][1])\n",
    "\n",
    "    if self.transforms!=None:\n",
    "      image = self.transforms(image)\n",
    "    return(image, label)\n",
    "    \n",
    "    \n",
    " #  creating pytorch datasets\n",
    "training_data = CustomCatsvsDogs(training_images, transforms=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
    "validation_data = CustomCatsvsDogs(validation_images, transforms=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convnet classes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ConvolutionalNeuralNet_2():\n",
    "  def __init__(self, network):\n",
    "    self.network = network.to(device)\n",
    "    self.optimizer = torch.optim.Adam(self.network.parameters(), lr=1e-3)\n",
    "\n",
    "  def train(self, loss_function, epochs, batch_size, \n",
    "            training_set, validation_set):\n",
    "    \n",
    "    #  creating log\n",
    "    log_dict = {\n",
    "        'training_loss_per_batch': [],\n",
    "        'validation_loss_per_batch': [],\n",
    "        'training_accuracy_per_epoch': [],\n",
    "        'training_recall_per_epoch': [],\n",
    "        'training_precision_per_epoch': [],\n",
    "        'validation_accuracy_per_epoch': [],\n",
    "        'validation_recall_per_epoch': [],\n",
    "        'validation_precision_per_epoch': []\n",
    "    } \n",
    "\n",
    "    #  defining weight initialization function\n",
    "    def init_weights(module):\n",
    "      if isinstance(module, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(module.weight)\n",
    "        module.bias.data.fill_(0.01)\n",
    "      elif isinstance(module, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(module.weight)\n",
    "        module.bias.data.fill_(0.01)\n",
    "\n",
    "    #  defining accuracy function\n",
    "    def accuracy(network, dataloader):\n",
    "      network.eval()\n",
    "      \n",
    "      all_predictions = []\n",
    "      all_labels = []\n",
    "\n",
    "      #  computing accuracy\n",
    "      total_correct = 0\n",
    "      total_instances = 0\n",
    "      for images, labels in tqdm(dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        all_labels.extend(labels)\n",
    "        predictions = torch.argmax(network(images), dim=1)\n",
    "        all_predictions.extend(predictions)\n",
    "        correct_predictions = sum(predictions==labels).item()\n",
    "        total_correct+=correct_predictions\n",
    "        total_instances+=len(images)\n",
    "      accuracy = round(total_correct/total_instances, 3)\n",
    "\n",
    "      #  computing recall and precision\n",
    "      true_positives = 0\n",
    "      false_negatives = 0\n",
    "      false_positives = 0\n",
    "      for idx in range(len(all_predictions)):\n",
    "        if all_predictions[idx].item()==1 and  all_labels[idx].item()==1:\n",
    "          true_positives+=1\n",
    "        elif all_predictions[idx].item()==0 and all_labels[idx].item()==1:\n",
    "          false_negatives+=1\n",
    "        elif all_predictions[idx].item()==1 and all_labels[idx].item()==0:\n",
    "          false_positives+=1\n",
    "      try:\n",
    "        recall = round(true_positives/(true_positives + false_negatives), 3)\n",
    "      except ZeroDivisionError:\n",
    "        recall = 0.0\n",
    "      try:\n",
    "        precision = round(true_positives/(true_positives + false_positives), 3)\n",
    "      except ZeroDivisionError:\n",
    "        precision = 0.0\n",
    "      return accuracy, recall, precision\n",
    "\n",
    "    #  initializing network weights\n",
    "    self.network.apply(init_weights)\n",
    "\n",
    "    #  creating dataloaders\n",
    "    train_loader = DataLoader(training_set, batch_size)\n",
    "    val_loader = DataLoader(validation_set, batch_size)\n",
    "\n",
    "    #  setting convnet to training mode\n",
    "    self.network.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      print(f'Epoch {epoch+1}/{epochs}')\n",
    "      train_losses = []\n",
    "\n",
    "      #  training\n",
    "      print('training...')\n",
    "      for images, labels in tqdm(train_loader):\n",
    "        #  sending data to device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        #  resetting gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        #  making predictions\n",
    "        predictions = self.network(images)\n",
    "        #  computing loss\n",
    "        loss = loss_function(predictions, labels)\n",
    "        log_dict['training_loss_per_batch'].append(loss.item())\n",
    "        train_losses.append(loss.item())\n",
    "        #  computing gradients\n",
    "        loss.backward()\n",
    "        #  updating weights\n",
    "        self.optimizer.step()\n",
    "      with torch.no_grad():\n",
    "        print('deriving training accuracy...')\n",
    "        #  computing training accuracy\n",
    "        train_accuracy, train_recall, train_precision = accuracy(self.network, train_loader)\n",
    "        log_dict['training_accuracy_per_epoch'].append(train_accuracy)\n",
    "        log_dict['training_recall_per_epoch'].append(train_recall)\n",
    "        log_dict['training_precision_per_epoch'].append(train_precision)\n",
    "\n",
    "      #  validation\n",
    "      print('validating...')\n",
    "      val_losses = []\n",
    "\n",
    "      #  setting convnet to evaluation mode\n",
    "      self.network.eval()\n",
    "\n",
    "      with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader):\n",
    "          #  sending data to device\n",
    "          images, labels = images.to(device), labels.to(device)\n",
    "          #  making predictions\n",
    "          predictions = self.network(images)\n",
    "          #  computing loss\n",
    "          val_loss = loss_function(predictions, labels)\n",
    "          log_dict['validation_loss_per_batch'].append(val_loss.item())\n",
    "          val_losses.append(val_loss.item())\n",
    "        #  computing accuracy\n",
    "        print('deriving validation accuracy...')\n",
    "        val_accuracy, val_recall, val_precision = accuracy(self.network, val_loader)\n",
    "        log_dict['validation_accuracy_per_epoch'].append(val_accuracy)\n",
    "        log_dict['validation_recall_per_epoch'].append(val_recall)\n",
    "        log_dict['validation_precision_per_epoch'].append(val_precision)\n",
    "\n",
    "      train_losses = np.array(train_losses).mean()\n",
    "      val_losses = np.array(val_losses).mean()\n",
    "\n",
    "      print(f'training_loss: {round(train_losses, 4)}  training_accuracy: '+\n",
    "      f'{train_accuracy}  training_recall: {train_recall}  training_precision: {train_precision} *~* validation_loss: {round(val_losses, 4)} '+  \n",
    "      f'validation_accuracy: {val_accuracy}  validation_recall: {val_recall}  validation_precision: {val_precision}\\n')\n",
    "      \n",
    "    return log_dict\n",
    "\n",
    "  def predict(self, x):\n",
    "    return self.network(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ConvNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(3, 8, 3, padding=1)\n",
    "    self.batchnorm1 = nn.BatchNorm2d(8)\n",
    "    self.conv2 = nn.Conv2d(8, 8, 3, padding=1)\n",
    "    self.batchnorm2 = nn.BatchNorm2d(8)\n",
    "    self.pool2 = nn.MaxPool2d(2)\n",
    "    self.conv3 = nn.Conv2d(8, 32, 3, padding=1)\n",
    "    self.batchnorm3 = nn.BatchNorm2d(32)\n",
    "    self.conv4 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "    self.batchnorm4 = nn.BatchNorm2d(32)\n",
    "    self.pool4 = nn.MaxPool2d(2)\n",
    "    self.conv5 = nn.Conv2d(32, 128, 3, padding=1)\n",
    "    self.batchnorm5 = nn.BatchNorm2d(128)\n",
    "    self.conv6 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "    self.batchnorm6 = nn.BatchNorm2d(128)\n",
    "    self.pool6 = nn.MaxPool2d(2)\n",
    "    self.conv7 = nn.Conv2d(128, 2, 1)\n",
    "    self.pool7 = nn.AvgPool2d(3)\n",
    "\n",
    "  def forward(self, x):\n",
    "    #-------------\n",
    "    # INPUT\n",
    "    #-------------\n",
    "    x = x.view(-1, 3, 32, 32)\n",
    "    \n",
    "    #-------------\n",
    "    # LAYER 1\n",
    "    #-------------\n",
    "    output_1 = self.conv1(x)\n",
    "    output_1 = F.relu(output_1)\n",
    "    output_1 = self.batchnorm1(output_1)\n",
    "\n",
    "    #-------------\n",
    "    # LAYER 2\n",
    "    #-------------\n",
    "    output_2 = self.conv2(output_1)\n",
    "    output_2 = F.relu(output_2)\n",
    "    output_2 = self.pool2(output_2)\n",
    "    output_2 = self.batchnorm2(output_2)\n",
    "\n",
    "    #-------------\n",
    "    # LAYER 3\n",
    "    #-------------\n",
    "    output_3 = self.conv3(output_2)\n",
    "    output_3 = F.relu(output_3)\n",
    "    output_3 = self.batchnorm3(output_3)\n",
    "\n",
    "    #-------------\n",
    "    # LAYER 4\n",
    "    #-------------\n",
    "    output_4 = self.conv4(output_3)\n",
    "    output_4 = F.relu(output_4)\n",
    "    output_4 = self.pool4(output_4)\n",
    "    output_4 = self.batchnorm4(output_4)\n",
    "\n",
    "    #-------------\n",
    "    # LAYER 5\n",
    "    #-------------\n",
    "    output_5 = self.conv5(output_4)\n",
    "    output_5 = F.relu(output_5)\n",
    "    output_5 = self.batchnorm5(output_5)\n",
    "\n",
    "    #-------------\n",
    "    # LAYER 6\n",
    "    #-------------\n",
    "    output_6 = self.conv6(output_5)\n",
    "    output_6 = F.relu(output_6)\n",
    "    output_6 = self.pool6(output_6)\n",
    "    output_6 = self.batchnorm6(output_6)\n",
    "\n",
    "    #--------------\n",
    "    # OUTPUT LAYER\n",
    "    #--------------\n",
    "    output_7 = self.conv7(output_6)\n",
    "    output_7 = self.pool7(output_7)\n",
    "    output_7 = output_7.view(-1, 2)\n",
    "\n",
    "    return F.softmax(output_7, dim=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training a Convolutional Neural Network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  training model\n",
    "model = ConvolutionalNeuralNet_2(ConvNet())\n",
    "\n",
    "log_dict = model.train(nn.CrossEntropyLoss(), epochs=10, batch_size=64, \n",
    "                       training_set=training_data, validation_set=validation_data)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}